{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60733ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import Counter\n",
    "\n",
    "def read_df(path):\n",
    "    for i in range (0,100):\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding='ISO-8859-1',skiprows=i)\n",
    "            \n",
    "            if \"date\" in \" \".join(list(df.columns)).lower() :\n",
    "                break\n",
    "        except:\n",
    "            None\n",
    "    df = pd.read_csv(path, encoding='ISO-8859-1',skiprows=i, on_bad_lines='skip')\n",
    "    return df\n",
    "def process_date(x):\n",
    "    if \"-\" in x:\n",
    "        return x.replace(\"-\",\"/\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def find_dt(dframe):\n",
    "    for i in dframe.columns:\n",
    "        if \"date\" in i.lower() and \"time\" in i.lower():\n",
    "            return i\n",
    "def find_date(dframe):\n",
    "    for i in dframe.columns:\n",
    "        if \"date\" in i.lower() and \"time\" not in i.lower():\n",
    "            return i\n",
    "def find_time(dframe):\n",
    "    for i in dframe.columns:\n",
    "        if \"date\" not in i.lower() and \"time\" in i.lower():\n",
    "            return i\n",
    "\n",
    "def find_temp(dframe):\n",
    "    for i in dframe.columns:\n",
    "        if \"temp\" in i.lower():\n",
    "            return i\n",
    "def find_conductivity(dframe):\n",
    "    for i in dframe.columns:\n",
    "        if \"conductivity\" in i.lower() and \"specific\" not in i.lower():\n",
    "            return i\n",
    "            \n",
    "def find_spec(dframe):\n",
    "    for i in dframe.columns:\n",
    "        if \"spec\" in i.lower() and \"cond\" in i.lower():\n",
    "            return i\n",
    "            \n",
    "def find_level(dframe):\n",
    "    for i in dframe.columns:\n",
    "        if \"level\" in i.lower():\n",
    "            return i\n",
    "            \n",
    "def find_word(dframe,word):\n",
    "    for i in dframe.columns:\n",
    "        if word in i.lower():\n",
    "            return i\n",
    "\n",
    "def find_master(listfile, word):\n",
    "    for i in listfile:\n",
    "        if word in i.split(\"/\")[-1]:\n",
    "            return i\n",
    "\n",
    "def find_datetime_format(a_dataframe_series, mode=None):\n",
    "    fir = list(set(a_dataframe_series.apply(lambda x: x.split(\" \")[0].split(\"/\")[0])))\n",
    "    sec = list(set(a_dataframe_series.apply(lambda x: x.split(\" \")[0].split(\"/\")[1])))\n",
    "    thi = list(set(a_dataframe_series.apply(lambda x: x.split(\" \")[0].split(\"/\")[2])))\n",
    "    dtset =[fir,sec,thi]\n",
    "    datetimefdict = {}\n",
    "    for i in range(3):\n",
    "        if len(dtset[i][0])==4:\n",
    "            datetimefdict[i]='%Y'\n",
    "        else:\n",
    "            if len(dtset[i])>12:\n",
    "                datetimefdict[i]='%d'\n",
    "            elif len(dtset[i])>2:\n",
    "                datetimefdict[i]='%m'\n",
    "            else:\n",
    "                datetimefdict[i]='%Y'\n",
    "    counter = Counter(datetimefdict)\n",
    "    date_format_list = [i[-1] for i in sorted(counter.items())]\n",
    "    if mode == \"datetime\":\n",
    "        return \"/\".join(date_format_list)+ \" %H:%M:%S\"\n",
    "    else:\n",
    "        return \"/\".join(date_format_list)\n",
    "\n",
    "\n",
    "allcol = ['DatasetName', 'MonitoringLocationID', 'MonitoringLocationName',\n",
    "       'MonitoringLocationLatitude', 'MonitoringLocationLongitude',\n",
    "       'MonitoringLocationHorizontalCoordinateReferenceSystem',\n",
    "       'MonitoringLocationHorizontalAccuracyMeasure',\n",
    "       'MonitoringLocationHorizontalAccuracyUnit', 'MonitoringLocationType',\n",
    "       'ActivityType', 'ActivityMediaName', 'ActivityStartDate',\n",
    "       'ActivityStartTime', 'ActivityStartTimeZone',\n",
    "       'SampleCollectionEquipmentName', 'CharacteristicName', 'ResultValue',\n",
    "       'ResultUnit', 'ResultValueType', 'ResultComment', 'DateTime']\n",
    "\n",
    "finalcol = ['DatasetName', 'MonitoringLocationID', 'MonitoringLocationName',\n",
    "       'MonitoringLocationLatitude', 'MonitoringLocationLongitude',\n",
    "       'MonitoringLocationHorizontalCoordinateReferenceSystem',\n",
    "       'MonitoringLocationHorizontalAccuracyMeasure',\n",
    "       'MonitoringLocationHorizontalAccuracyUnit', 'MonitoringLocationType',\n",
    "       'ActivityType', 'ActivityMediaName', 'ActivityStartDate',\n",
    "       'ActivityStartTime', 'ActivityStartTimeZone',\n",
    "       'SampleCollectionEquipmentName', 'CharacteristicName', 'ResultValue',\n",
    "       'ResultUnit', 'ResultValueType', 'ResultComment']\n",
    "\n",
    "fixcol = ['DatasetName', 'MonitoringLocationID', 'MonitoringLocationName',\n",
    "       'MonitoringLocationLatitude', 'MonitoringLocationLongitude',\n",
    "       'MonitoringLocationHorizontalCoordinateReferenceSystem',\n",
    "       'MonitoringLocationHorizontalAccuracyMeasure',\n",
    "       'MonitoringLocationHorizontalAccuracyUnit', 'MonitoringLocationType',\n",
    "       'ActivityType', 'ActivityMediaName', 'ActivityStartTimeZone',\n",
    "       'SampleCollectionEquipmentName', 'CharacteristicName',\n",
    "        'ResultUnit', 'ResultValueType', 'ResultComment']\n",
    "\n",
    "updatecol = ['ActivityStartDate','ActivityStartTime','ResultValue','DateTime']\n",
    "def create_master(adf, adfmaster, typeword):\n",
    "    df_new = pd.DataFrame()\n",
    "    for i in allcol:\n",
    "        df_new[i]=[adfmaster[i][0] for _ in range(len(adf))]\n",
    "    \n",
    "    adf['ActivityStartDate'] = adf['DateTime'].apply(lambda x:  x.strftime('%Y-%m-%d'))\n",
    "    adf['ActivityStartTime'] = adf['DateTime'].apply(lambda x:  x.strftime('%H:%M:%S'))\n",
    "    \n",
    "    df_new['ActivityStartDate'] = adf['ActivityStartDate']\n",
    "    df_new['ActivityStartTime'] = adf['ActivityStartTime']\n",
    "    df_new['ResultValue'] = adf[find_word(adf, typeword)]\n",
    "    df_new['DateTime'] = adf['DateTime']\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def drop_error(adf):\n",
    "    if \"ERROR\" in adf.columns:\n",
    "        return adf.drop(columns=['ERROR'])\n",
    "        \n",
    "    else:\n",
    "        return adf\n",
    "\n",
    "def find_error(adf):\n",
    "    \n",
    "    adf['ERROR']= None\n",
    "    \n",
    "    if find_dt(adf):\n",
    "        dt_col= find_dt(adf)\n",
    "        for idx in range(len(adf[dt_col])):\n",
    "            try:\n",
    "                parser.parse(adf[dt_col].loc[idx])\n",
    "            except:\n",
    "               adf.loc[idx, \"ERROR\"] = \"DATETIME ERROR\"\n",
    "    else:\n",
    "        date = find_date(adf)\n",
    "        time = find_time(adf)\n",
    "        for idx in range(len(adf)):\n",
    "            try:\n",
    "                parser.parse(adf[date].loc[idx])\n",
    "            except:\n",
    "                adf.loc[idx, \"ERROR\"] = \"DATETIME ERROR\"\n",
    "\n",
    "            try:\n",
    "                parser.parse(adf[time].loc[idx])\n",
    "            except:\n",
    "                adf.loc[idx, \"ERROR\"] = \"DATETIME ERROR\"\n",
    "\n",
    "    return adf\n",
    "# path_hobo_solinst = '/home/datpham/cosmo/csv/type/Data files/'\n",
    "# path_master = '/home/datpham/cosmo/csv/type/'\n",
    "# listcsv = glob.glob(path_hobo_solinst+\"*\")\n",
    "# listmaster = glob.glob(path_master+\"*\")\n",
    "\n",
    "# listcsv = [i for i in listcsv if \"csv\" in i.split(\"/\")[-1]]\n",
    "# listmaster = [i for i in listmaster if \"xlsx\" in  i.split(\"/\")[-1]]\n",
    "\n",
    "def qaqc(list_site_file_link, list_master_file_link, working_path):\n",
    "    df_con =pd.DataFrame()\n",
    "    df_tem =pd.DataFrame()\n",
    "    df_spe =pd.DataFrame()\n",
    "    df_dep =pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for csvf in tqdm(list_site_file_link):\n",
    "    \n",
    "        dfcsv =read_df(csvf)\n",
    "        #dfcsv = dfcsv.loc[:, (dfcsv != 0).all(axis=0)]   # zeros column not retained\n",
    "        flag = True\n",
    "        \n",
    "        if find_dt(dfcsv):\n",
    "            #If this file contains date time in one column\n",
    "            dtc = find_dt(dfcsv)\n",
    "            dfcsv[dtc] = dfcsv[dtc].apply(lambda x: process_date(x) )\n",
    "\n",
    "            date_time_format = find_datetime_format(dfcsv[dtc], mode=\"datetime\")\n",
    "            # yearlength4digit = [1 if len(i)==4 else 0 for i in dfcsv[dtc][0].split(\" \")[0].split(\"/\") ]\n",
    "\n",
    "            # if sum(yearlength4digit)<1:\n",
    "\n",
    "            #     now = datetime.now()\n",
    "            #     year2digit = str(now.year)[:2]\n",
    "            #     dfcsv[dtc] = year2digit + dfcsv[dtc]\n",
    "            \n",
    "            try:\n",
    "                # dfcsv['DateTime'] = dfcsv[dtc].apply(lambda x:  parser.parse(x))\n",
    "                dfcsv['DateTime'] = dfcsv[dtc].apply(lambda x:  datetime.strptime(x,date_time_format))\n",
    "                \n",
    "                remove_num= int(timedelta(hours=3)/(dfcsv['DateTime'].iloc[1] - dfcsv['DateTime'].iloc[0]))\n",
    "                dfcsv = dfcsv.iloc[:len(dfcsv)-remove_num]\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                \n",
    "                # shutil.move(csvf, \"/\".join(csvf.split(\"/\")[:-1])+ \"/outlier/\" + csvf.split(\"/\")[-1])\n",
    "                dfcsv = find_error(dfcsv)\n",
    "                dfcsv.to_csv(working_path + \"/site_files_outlier/\" + csvf.split(\"/\")[-1] ,index= False) \n",
    "                flag = False\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            # IF this file have seperate date and time columns\n",
    "            \n",
    "            date = find_date(dfcsv)\n",
    "            dfcsv[date] = dfcsv[date].apply(lambda x: process_date(x) )\n",
    "            date_format = find_datetime_format( dfcsv[date], mode=None)\n",
    "\n",
    "            time = find_time(dfcsv)\n",
    "            try:\n",
    "                # dfcsv[\"Date\"] = dfcsv['Date'].apply(lambda x:  parser.parse(x))\n",
    "                dfcsv[\"Date\"] = dfcsv['Date'].apply(lambda x: datetime.strptime(x,date_format))\n",
    "                \n",
    "                    \n",
    "                dfcsv[\"Time\"] = dfcsv['Time'].apply(lambda x:  parser.parse(x))\n",
    "    \n",
    "                dfcsv['Date_Time'] = dfcsv[\"Date\"].apply(lambda x:  x.strftime('%Y-%m-%d')) + \" \" + dfcsv[\"Time\"].apply(lambda x: x.strftime('%H:%M:%S'))\n",
    "                dfcsv['DateTime'] = dfcsv[\"Date_Time\"].apply(lambda x:  datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "                \n",
    "                \n",
    "            except:\n",
    "                \n",
    "                flag = False\n",
    "    \n",
    "                dfcsv = find_error(dfcsv)\n",
    "                dfcsv.to_csv( working_path + \"/site_files_outlier/\" + csvf.split(\"/\")[-1], index= False)  \n",
    "                # shutil.move(csvf, \"/\".join(csvf.split(\"/\")[:-1])+ \"/outlier/\" + csvf.split(\"/\")[-1])\n",
    "                \n",
    "    \n",
    "    #find_dt find_date find_time find_temp find_conductivity find_spec find_level\n",
    "    \n",
    "        if flag:\n",
    "            con = find_conductivity(dfcsv) \n",
    "            if con:\n",
    "                df_con = pd.concat([df_con, dfcsv[['DateTime']+[con]]])\n",
    "    \n",
    "            spec = find_spec(dfcsv) \n",
    "            if spec:\n",
    "                df_spe = pd.concat([df_spe, dfcsv[['DateTime']+[spec]]])\n",
    "    \n",
    "            dep = find_level(dfcsv) \n",
    "            if dep:\n",
    "                dfcsv[dep] = dfcsv[dfcsv[dep]<=5][dep]\n",
    "                \n",
    "                df_dep = pd.concat([df_dep, dfcsv[['DateTime']+[dep]]])\n",
    "    \n",
    "            temp = find_temp(dfcsv) \n",
    "            if temp:\n",
    "                 \n",
    "                df_tem = pd.concat([df_tem, dfcsv[['DateTime']+[temp]]])\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        df_con_master = pd.read_csv(find_master(list_master_file_link,\"conductivity\"), low_memory=False)\n",
    "        \n",
    "        df_con_master[\"Date\"] = df_con_master['ActivityStartDate'].apply(lambda x:  datetime.strptime(x,'%m/%d/%Y'))\n",
    "    \n",
    "         # 'ActivityStartDate','ActivityStartTime',       \n",
    "                    \n",
    "        df_con_master[\"Time\"] = df_con_master['ActivityStartTime'].apply(lambda x:  parser.parse(x))\n",
    "    \n",
    "        df_con_master['Date_Time'] = df_con_master[\"Date\"].apply(lambda x:  x.strftime('%Y-%m-%d')) + \" \" + df_con_master[\"Time\"].apply(lambda x: x.strftime('%H:%M:%S'))\n",
    "        \n",
    "        df_con_master['DateTime'] = df_con_master[\"Date_Time\"].apply(lambda x:  datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "            \n",
    "    except:\n",
    "        df_con_master = pd.DataFrame()\n",
    "    try:\n",
    "        df_spe_master = pd.read_csv(find_master(list_master_file_link,\"spec\"), low_memory=False)\n",
    "        \n",
    "        df_spe_master[\"Date\"] = df_spe_master['ActivityStartDate'].apply(lambda x:  datetime.strptime(x,'%m/%d/%Y'))\n",
    "    \n",
    "         # 'ActivityStartDate','ActivityStartTime',       \n",
    "                    \n",
    "        df_spe_master[\"Time\"] = df_spe_master['ActivityStartTime'].apply(lambda x:  parser.parse(x))\n",
    "    \n",
    "        df_spe_master['Date_Time'] = df_spe_master[\"Date\"].apply(lambda x:  x.strftime('%Y-%m-%d')) + \" \" + df_spe_master[\"Time\"].apply(lambda x: x.strftime('%H:%M:%S'))\n",
    "        \n",
    "        df_spe_master['DateTime'] = df_spe_master[\"Date_Time\"].apply(lambda x:  datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    \n",
    "    except:\n",
    "        df_spe_master =pd.DataFrame()\n",
    "    try:\n",
    "        df_tem_master = pd.read_csv(find_master(list_master_file_link,\"temp\"), low_memory=False)\n",
    "\n",
    "        df_tem_master[\"Date\"] = df_tem_master['ActivityStartDate'].apply(lambda x:  datetime.strptime(x,'%m/%d/%Y'))\n",
    "    \n",
    "         # 'ActivityStartDate','ActivityStartTime',       \n",
    "                    \n",
    "        df_tem_master[\"Time\"] = df_tem_master['ActivityStartTime'].apply(lambda x:  parser.parse(x))\n",
    "    \n",
    "        df_tem_master['Date_Time'] = df_tem_master[\"Date\"].apply(lambda x:  x.strftime('%Y-%m-%d')) + \" \" + df_tem_master[\"Time\"].apply(lambda x: x.strftime('%H:%M:%S'))\n",
    "        \n",
    "        df_tem_master['DateTime'] = df_tem_master[\"Date_Time\"].apply(lambda x:  datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "        \n",
    "    except:\n",
    "        df_tem_master = pd.DataFrame()\n",
    "    try:\n",
    "        \n",
    "        df_dep_master = pd.read_csv(find_master(list_master_file_link,\"depth\"), low_memory=False)\n",
    "        \n",
    "        df_dep_master[\"Date\"] = df_dep_master['ActivityStartDate'].apply(lambda x:  datetime.strptime(x,'%m/%d/%Y'))\n",
    "    \n",
    "         # 'ActivityStartDate','ActivityStartTime',       \n",
    "                    \n",
    "        df_dep_master[\"Time\"] = df_dep_master['ActivityStartTime'].apply(lambda x:  parser.parse(x))\n",
    "    \n",
    "        df_dep_master['Date_Time'] = df_dep_master[\"Date\"].apply(lambda x:  x.strftime('%Y-%m-%d')) + \" \" + df_dep_master[\"Time\"].apply(lambda x: x.strftime('%H:%M:%S'))\n",
    "        \n",
    "        df_dep_master['DateTime'] = df_dep_master[\"Date_Time\"].apply(lambda x:  datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "    except:\n",
    "        df_dep_master = pd.DataFrame()\n",
    "   \n",
    "    \n",
    "    if df_con.empty == False and df_con_master.empty == False: \n",
    "        df_con = drop_error(df_con)    \n",
    "        df_con = df_con.sort_values(by='DateTime').reset_index()\n",
    "        df_con = df_con.drop(columns=['index'])\n",
    "        df_con = df_con.drop_duplicates(subset='DateTime', keep=\"first\") \n",
    "        \n",
    "        df_con_new = create_master(df_con,df_con_master,\"conductivity\")\n",
    "        df_con_new = df_con_new.dropna(axis=0,subset=['ResultValue'])\n",
    "        df_con_new = df_con_new[df_con_new['DateTime']>df_con_master['DateTime'].iloc[-1]]\n",
    "        \n",
    "\n",
    "        if df_con_new.empty == False:\n",
    "            con_name = find_master(list_master_file_link,\"conductivity\").split(\"/\")[-1]\n",
    "\n",
    "            df_con_new[allcol].to_csv(working_path + \"/post_processed_files_before_merge_new_data_only/\" + con_name ,index= False )\n",
    "            df_con_new =  pd.concat([df_con_new,df_con_master])\n",
    "            df_con_new = df_con_new.sort_values(by='DateTime')\n",
    "    \n",
    "            df_con_new[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + con_name ,index= False )  \n",
    "        else:\n",
    "        \n",
    "            con_name = find_master(list_master_file_link,\"conductivity\").split(\"/\")[-1]\n",
    "            df_con_master[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + con_name ,index= False )  \n",
    "    elif df_con_master.empty == False:\n",
    "        \n",
    "        con_name = find_master(list_master_file_link,\"conductivity\").split(\"/\")[-1]\n",
    "        df_con_master[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + con_name ,index= False )\n",
    "        \n",
    "        \n",
    "    if df_spe.empty == False and  df_spe_master.empty == False:\n",
    "        df_spe = drop_error(df_spe)\n",
    "        df_spe = df_spe.sort_values(by='DateTime').reset_index()\n",
    "        df_spe = df_spe.drop(columns=['index'])\n",
    "        df_spe = df_spe.drop_duplicates(subset='DateTime', keep=\"first\")\n",
    "        \n",
    "        df_spe_new = create_master(df_spe,df_spe_master,\"spec\")\n",
    "        df_spe_new = df_spe_new.dropna(axis=0,subset=['ResultValue'])\n",
    "        df_spe_new = df_spe_new[df_spe_new['DateTime']>df_spe_master['DateTime'].iloc[-1]]\n",
    "        \n",
    "        \n",
    "\n",
    "        if df_spe_new.empty == False:\n",
    "            spe_name = find_master(list_master_file_link,\"spec\").split(\"/\")[-1]\n",
    "            df_spe_new[allcol].to_csv(working_path + \"/post_processed_files_before_merge_new_data_only/\" + spe_name ,index= False )\n",
    "            \n",
    "            df_spe_new =  pd.concat([df_spe_new,df_spe_master])\n",
    "            df_spe_new = df_spe_new.sort_values(by='DateTime')\n",
    "            \n",
    "            df_spe_new[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + spe_name ,index= False )  \n",
    "        else:\n",
    "            spe_name = find_master(list_master_file_link,\"spec\").split(\"/\")[-1]\n",
    "            \n",
    "            df_spe_master[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + spe_name ,index= False )\n",
    "    elif df_spe_master.empty == False:\n",
    "        \n",
    "        spe_name = find_master(list_master_file_link,\"spec\").split(\"/\")[-1]\n",
    "            \n",
    "        df_spe_master[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + spe_name ,index= False )\n",
    "    \n",
    "            \n",
    "    if df_dep.empty == False and df_dep_master.empty == False :\n",
    "        df_dep = drop_error(df_dep)\n",
    "        df_dep = df_dep.sort_values(by='DateTime').reset_index()\n",
    "        df_dep = df_dep.drop(columns=['index'])\n",
    "        df_dep = df_dep.drop_duplicates(subset='DateTime', keep=\"first\")   \n",
    "        \n",
    "        df_dep_new = create_master(df_dep,df_dep_master,\"level\")\n",
    "        df_dep_new = df_dep_new.dropna(axis=0,subset=['ResultValue'])\n",
    "        df_dep_new = df_dep_new[df_dep_new['DateTime']>df_dep_master['DateTime'].iloc[-1]]\n",
    "        \n",
    "\n",
    "        if df_dep_new.empty == False:\n",
    "            dep_name = find_master(list_master_file_link,\"depth\").split(\"/\")[-1]\n",
    "            df_dep_new[allcol].to_csv(working_path + \"/post_processed_files_before_merge_new_data_only/\" + dep_name ,index= False)\n",
    "            \n",
    "            df_dep_new =  pd.concat([df_dep_new,df_dep_master])\n",
    "            df_dep_new = df_dep_new.sort_values(by='DateTime')\n",
    "\n",
    "            df_dep_new[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + dep_name ,index= False)\n",
    "        else:\n",
    "            dep_name = find_master(list_master_file_link,\"depth\").split(\"/\")[-1]\n",
    "            df_dep_master[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + dep_name, index= False)\n",
    "\n",
    "    elif  df_dep_master.empty == False:\n",
    "        dep_name = find_master(list_master_file_link,\"depth\").split(\"/\")[-1]\n",
    "        df_dep_master[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + dep_name, index= False)\n",
    "    \n",
    "        \n",
    "    if df_tem.empty == False and df_tem_master.empty == False :\n",
    "        df_tem = drop_error(df_tem)\n",
    "        df_tem = df_tem.sort_values(by='DateTime').reset_index()\n",
    "        df_tem = df_tem.drop(columns=['index'])\n",
    "        df_tem = df_tem.drop_duplicates(subset='DateTime', keep=\"first\")  \n",
    "        \n",
    "        df_tem_new = create_master(df_tem,df_tem_master,\"temp\")\n",
    "\n",
    "        df_tem_new = df_tem_new.dropna(axis=0,subset=['ResultValue'])\n",
    "        df_tem_new = df_tem_new[df_tem_new['DateTime']>df_tem_master['DateTime'].iloc[-1]]\n",
    "\n",
    "        \n",
    "\n",
    "        if df_tem_new.empty == False:\n",
    "            tem_name = find_master(list_master_file_link,\"temp\").split(\"/\")[-1]\n",
    "            df_tem_new[allcol].to_csv(working_path + \"/post_processed_files_before_merge_new_data_only/\" + tem_name, index= False)\n",
    "\n",
    "            df_tem_new =  pd.concat([df_tem_new,df_tem_master])\n",
    "            df_tem_new = df_tem_new.sort_values(by='DateTime')\n",
    "\n",
    "            df_tem_new[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + tem_name, index= False)\n",
    "        else:\n",
    "            tem_name = find_master(list_master_file_link,\"temp\").split(\"/\")[-1]\n",
    "            df_tem_master[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + tem_name, index= False)  \n",
    "    elif  df_tem_master.empty == False:\n",
    "        tem_name = find_master(list_master_file_link,\"temp\").split(\"/\")[-1]\n",
    "        df_tem_master[allcol].to_csv(working_path + \"/post_processed_files_before_merge_appended_old_data/\" + tem_name, index= False)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_path  = '/lakehouse/default/Files'\n",
    "list_csv_link = glob.glob(work_path + \"/site_files/*\")\n",
    "list_master_link = glob.glob(work_path +\"/master_files/*\")\n",
    "\n",
    "print(\"NUMBER OF SITE FILES: \",len(list_csv_link))\n",
    "print(\"NUMBER OF MASTER FILES: \",len(list_master_link))\n",
    "# list_csv_link = [i for i in list_csv_link if \"csv\" in i.split(\"/\")[-1]]\n",
    "# list_master_link = [i for i in list_master_link if \"csv\" in  i.split(\"/\")[-1]]\n",
    "\n",
    "site_name_from_csv_list = [i.split(\"/\")[-1].split(\" - \")[0] for i in list_csv_link]\n",
    "\n",
    "site_name_from_master_list = [i.split(\"/\")[-1].split(\" - \")[0] for i in list_master_link]\n",
    "\n",
    "Site_name_from_site_folder_dict ={}\n",
    "for site in set(site_name_from_csv_list):\n",
    "    Site_name_from_site_folder_dict[site] = [list_csv_link[idx] for idx,name in enumerate(site_name_from_csv_list) if name == site]\n",
    "\n",
    "    \n",
    "Site_name_from_master_folder_dict ={}\n",
    "for site in set(site_name_from_master_list):\n",
    "    Site_name_from_master_folder_dict[site] = [list_master_link[idx] for idx,name in enumerate(site_name_from_master_list) if name == site]\n",
    "\n",
    "\n",
    "for site in tqdm(Site_name_from_site_folder_dict.keys()):\n",
    "    \n",
    "    same_site_file_link = Site_name_from_site_folder_dict.get(site)\n",
    "    \n",
    "    same_site_master_file_link = Site_name_from_master_folder_dict.get(site)\n",
    "    if same_site_master_file_link and same_site_file_link:\n",
    "        print(\"There are \",len(same_site_file_link), \"site files\", \"for this \",site,\". Processing Them\")\n",
    "        qaqc(same_site_file_link, same_site_master_file_link, work_path)\n",
    "    \n",
    "\n",
    "\n",
    "path_before_merge = work_path + '/post_processed_files_before_merge_new_data_only/'\n",
    "listcsv_before_merge = glob.glob(path_before_merge+\"*\")\n",
    "\n",
    "df_merged =  pd.DataFrame()\n",
    "print(\"PROCESSING SITE FILES COMPLETED, MERGING THEM AND SAVING FINAL FILE...\")\n",
    "for file in listcsv_before_merge:\n",
    "    df_before_merge = pd.read_csv(file)\n",
    "    df_merged= pd.concat([df_merged,df_before_merge])\n",
    "    df_merged = df_merged.sort_values(by=['MonitoringLocationID','DateTime'])\n",
    "    \n",
    "    df_merged = df_merged.drop_duplicates()\n",
    "    df_merged = df_merged.dropna(axis=0,subset=['ResultValue'])\n",
    "    df_merged[finalcol].to_csv(work_path + \"/final_file_ready_to_upload/final_master_file.csv\", index= False)\n",
    "    \n",
    "print(\"...SAVING COMPLETED, JOB DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551bb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "# from googleapiclient.discovery import build\n",
    "# from google.oauth2 import service_account\n",
    "\n",
    "# creds = service_account.Credentials.from_service_account_file(\n",
    "#     \"/lakehouse/default/Files/clients_secrets.json\",\n",
    "#     scopes=['https://www.googleapis.com/auth/drive']\n",
    "# )\n",
    "\n",
    "# drive_service = build('drive', 'v3', credentials=creds)\n",
    "# results = (\n",
    "#         drive_service.files()\n",
    "#         .list(pageSize=1000)#, fields=\"nextPageToken, files(id, name)\")\n",
    "#         .execute()\n",
    "        \n",
    "#     )\n",
    "# items = results.get(\"files\", [])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
